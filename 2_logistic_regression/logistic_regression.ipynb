{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato de X_train: (455, 31)\n",
      "Formato de y_train: (455,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Carregar o dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data  # Características\n",
    "y = data.target  # Rótulos (0 ou 1)\n",
    "\n",
    "# Dividir o dataset em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Adicionar uma coluna de 1s para o termo de bias (intercept)\n",
    "X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "\n",
    "print(\"Formato de X_train:\", X_train.shape)\n",
    "print(\"Formato de y_train:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou seja, temos 31 informações para cada amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danys\\AppData\\Local\\Temp\\ipykernel_12648\\1426480214.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parâmetros finais (theta): [ 4.54128011e-01  3.43661199e+00  1.97182059e+00  1.87050641e+01\n",
      "  4.28122849e+00  2.28499157e-02 -5.24155871e-02 -1.14175776e-01\n",
      " -4.58061737e-02  4.33981110e-02  2.09675484e-02 -4.55157746e-04\n",
      "  1.64043533e-01 -3.43019950e-01 -1.02682653e+01  2.34461459e-04\n",
      " -1.45640295e-02 -2.16321829e-02 -4.29297975e-03  1.50529971e-03\n",
      " -4.94895626e-04  3.68131186e+00  1.85292296e+00  1.80934537e+01\n",
      " -7.51588569e+00  2.23040012e-02 -1.93294780e-01 -2.89313736e-01\n",
      " -6.88360311e-02  3.37556817e-02  9.72950629e-03]\n",
      "Parâmetros finais (theta2): [ 4.54128011e-01  3.43661199e+00  1.97182059e+00  1.87050641e+01\n",
      "  4.28122849e+00  2.28499157e-02 -5.24155871e-02 -1.14175776e-01\n",
      " -4.58061737e-02  4.33981110e-02  2.09675484e-02 -4.55157746e-04\n",
      "  1.64043533e-01 -3.43019950e-01 -1.02682653e+01  2.34461459e-04\n",
      " -1.45640295e-02 -2.16321829e-02 -4.29297975e-03  1.50529971e-03\n",
      " -4.94895626e-04  3.68131186e+00  1.85292296e+00  1.80934537e+01\n",
      " -7.51588569e+00  2.23040012e-02 -1.93294780e-01 -2.89313736e-01\n",
      " -6.88360311e-02  3.37556817e-02  9.72950629e-03]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def GD(X, y, w, alpha, num_iters = 1000):\n",
    "    N = len(y)\n",
    "    for _ in range(num_iters):\n",
    "        # X.dot(w) vai retornar um vetor onde cada valor é w.T.dot(X[i]);\n",
    "        # y - esse vetor vai representar o vetor erro, no qual cada i-ésimo valor de 1 a N é o erro para amostra i\n",
    "        # ESSA PARTE DO NP.DOT EXTERNO EU NÃO ENTENDI\n",
    "        w += alpha * (1 / N) * np.dot( X.T, y - sigmoid(X.dot(w)) )\n",
    "    return w\n",
    "\n",
    "def GD2(X, y, w, alpha, num_iters):\n",
    "    N = len(y)  \n",
    "    for _ in range(num_iters):\n",
    "        gradient = np.zeros_like(w) # o gradiente é um vetor do tamanho do vetor de pesos\n",
    "\n",
    "        for i in range(N):\n",
    "            error_i = y[i] - sigmoid(np.dot(w, X[i])) \n",
    "            gradient += error_i * X[i] \n",
    "            # asterisco multiplica cada elemento de um vetor com o respectivo de outro vetor (em ordem)\n",
    "            # gradient = [error_i * Xi[1], error_i * Xi[2], ..., error_i * Xi[D]]\n",
    "\n",
    "        w += alpha * (1 / N) * gradient\n",
    "\n",
    "    return w\n",
    "\n",
    "w = np.zeros(X_train.shape[1])\n",
    "alpha = 0.01\n",
    "num_iters = 1000\n",
    "\n",
    "theta  = GD(X_train, y_train, w, alpha, num_iters)\n",
    "theta2 = GD2(X_train, y_train, w, alpha, num_iters)\n",
    "\n",
    "print(\"Parâmetros finais (theta):\",  theta)\n",
    "print(\"Parâmetros finais (theta2):\", theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia no conjunto de teste usando GD: 0.956140350877193\n",
      "Acurácia no conjunto de teste usando GD: 0.956140350877193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danys\\AppData\\Local\\Temp\\ipykernel_12648\\1426480214.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "def predict(X, theta):\n",
    "    return (sigmoid(X.dot(theta)) >= 0.5).astype(int)\n",
    "\n",
    "y_pred = predict(X_test, theta)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Acurácia no conjunto de teste usando GD:\", accuracy)\n",
    "\n",
    "y_pred = predict(X_test, theta2)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Acurácia no conjunto de teste usando GD:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10000,32) and (31,) not aligned: 32 (dim 1) != 31 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[0;32m     31\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 34\u001b[0m \u001b[43mplot_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpca\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36mplot_classification\u001b[1;34m(X_reduced, y, w, pca)\u001b[0m\n\u001b[0;32m     15\u001b[0m grid_original \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39minverse_transform(np\u001b[38;5;241m.\u001b[39mc_[xx1\u001b[38;5;241m.\u001b[39mravel(), xx2\u001b[38;5;241m.\u001b[39mravel()])\n\u001b[0;32m     16\u001b[0m grid_original \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((np\u001b[38;5;241m.\u001b[39mones((grid_original\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)), grid_original))  \n\u001b[1;32m---> 18\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_original\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m h \u001b[38;5;241m=\u001b[39m sigmoid(z)\n\u001b[0;32m     20\u001b[0m h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mreshape(xx1\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (10000,32) and (31,) not aligned: 32 (dim 1) != 31 (dim 0)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X_train[:, 1:])\n",
    "\n",
    "def plot_classification(X_reduced, y, w, pca):\n",
    "    \n",
    "    x1_min, x1_max = X_reduced[:, 0].min() - 1, X_reduced[:, 0].max() + 1\n",
    "    x2_min, x2_max = X_reduced[:, 1].min() - 1, X_reduced[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100), np.linspace(x2_min, x2_max, 100))\n",
    "    \n",
    "    grid_original = pca.inverse_transform(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "    grid_original = np.hstack((np.ones((grid_original.shape[0], 1)), grid_original))  \n",
    "    \n",
    "    z = grid_original.dot(w)\n",
    "    h = sigmoid(z)\n",
    "    h = h.reshape(xx1.shape)\n",
    "    \n",
    "    plt.contour(xx1, xx2, h, levels=[0.5], colors='black')\n",
    "    \n",
    "    plt.scatter(X_reduced[y == 0, 0], X_reduced[y == 0, 1], color='red', label='Classe 0')\n",
    "    plt.scatter(X_reduced[y == 1, 0], X_reduced[y == 1, 1], color='blue', label='Classe 1')\n",
    "\n",
    "    plt.xlabel('Componente Principal 1')\n",
    "    plt.ylabel('Componente Principal 2')\n",
    "    plt.title('Classificação e Fronteira de Decisão')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_classification(X_reduced, y_train, w, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
