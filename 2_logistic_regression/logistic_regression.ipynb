{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato de X_train: (455, 31)\n",
      "Formato de y_train: (455,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Carregar o dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data  # Características\n",
    "y = data.target  # Rótulos (0 ou 1)\n",
    "\n",
    "# Dividir o dataset em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Adicionar uma coluna de 1s para o termo de bias (intercept)\n",
    "X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "\n",
    "print(\"Formato de X_train:\", X_train.shape)\n",
    "print(\"Formato de y_train:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou seja, temos 31 informações para cada amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danys\\AppData\\Local\\Temp\\ipykernel_12648\\1426480214.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parâmetros finais (theta): [ 4.54128011e-01  3.43661199e+00  1.97182059e+00  1.87050641e+01\n",
      "  4.28122849e+00  2.28499157e-02 -5.24155871e-02 -1.14175776e-01\n",
      " -4.58061737e-02  4.33981110e-02  2.09675484e-02 -4.55157746e-04\n",
      "  1.64043533e-01 -3.43019950e-01 -1.02682653e+01  2.34461459e-04\n",
      " -1.45640295e-02 -2.16321829e-02 -4.29297975e-03  1.50529971e-03\n",
      " -4.94895626e-04  3.68131186e+00  1.85292296e+00  1.80934537e+01\n",
      " -7.51588569e+00  2.23040012e-02 -1.93294780e-01 -2.89313736e-01\n",
      " -6.88360311e-02  3.37556817e-02  9.72950629e-03]\n",
      "Parâmetros finais (theta2): [ 4.54128011e-01  3.43661199e+00  1.97182059e+00  1.87050641e+01\n",
      "  4.28122849e+00  2.28499157e-02 -5.24155871e-02 -1.14175776e-01\n",
      " -4.58061737e-02  4.33981110e-02  2.09675484e-02 -4.55157746e-04\n",
      "  1.64043533e-01 -3.43019950e-01 -1.02682653e+01  2.34461459e-04\n",
      " -1.45640295e-02 -2.16321829e-02 -4.29297975e-03  1.50529971e-03\n",
      " -4.94895626e-04  3.68131186e+00  1.85292296e+00  1.80934537e+01\n",
      " -7.51588569e+00  2.23040012e-02 -1.93294780e-01 -2.89313736e-01\n",
      " -6.88360311e-02  3.37556817e-02  9.72950629e-03]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def GD(X, y, w, alpha, num_iters = 1000):\n",
    "    N = len(y)\n",
    "    for _ in range(num_iters):\n",
    "        # X.dot(w) vai retornar um vetor onde cada valor é w.T.dot(X[i]);\n",
    "        # y - esse vetor vai representar o vetor erro, no qual cada i-ésimo valor de 1 a N é o erro para amostra i\n",
    "        # ESSA PARTE DO NP.DOT EXTERNO EU NÃO ENTENDI\n",
    "        w += alpha * (1 / N) * np.dot( X.T, y - sigmoid(X.dot(w)) )\n",
    "    return w\n",
    "\n",
    "def GD2(X, y, w, alpha, num_iters):\n",
    "    N = len(y)  \n",
    "    for _ in range(num_iters):\n",
    "        gradient = np.zeros_like(w) # o gradiente é um vetor do tamanho do vetor de pesos\n",
    "\n",
    "        for i in range(N):\n",
    "            error_i = y[i] - sigmoid(np.dot(w, X[i])) \n",
    "            gradient += error_i * X[i] \n",
    "            # asterisco multiplica cada elemento de um vetor com o respectivo de outro vetor (em ordem)\n",
    "            # gradient = [error_i * Xi[1], error_i * Xi[2], ..., error_i * Xi[D]]\n",
    "\n",
    "        w += alpha * (1 / N) * gradient\n",
    "\n",
    "    return w\n",
    "\n",
    "w = np.zeros(X_train.shape[1])\n",
    "alpha = 0.01\n",
    "num_iters = 1000\n",
    "\n",
    "theta  = GD(X_train, y_train, w, alpha, num_iters)\n",
    "theta2 = GD2(X_train, y_train, w, alpha, num_iters)\n",
    "\n",
    "print(\"Parâmetros finais (theta):\",  theta)\n",
    "print(\"Parâmetros finais (theta2):\", theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia no conjunto de teste usando GD: 0.956140350877193\n",
      "Acurácia no conjunto de teste usando GD: 0.956140350877193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danys\\AppData\\Local\\Temp\\ipykernel_12648\\1426480214.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "def predict(X, theta):\n",
    "    return (sigmoid(X.dot(theta)) >= 0.5).astype(int)\n",
    "\n",
    "y_pred = predict(X_test, theta)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Acurácia no conjunto de teste usando GD:\", accuracy)\n",
    "\n",
    "y_pred = predict(X_test, theta2)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Acurácia no conjunto de teste usando GD:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
